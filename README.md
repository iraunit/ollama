## Ollama Docker FastAPI

This is a simple Dockerized FastAPI application that returns a JSON response.
It uses ollama to run llama3.1 model inside a docker container and serve the model as a FastAPI application.

Just use the compose.yml.